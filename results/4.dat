I1112 01:20:37.008500  3508 caffe.cpp:211] Use CPU.
I1112 01:20:37.035107  3508 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_test_4.prototxt"
train_state {
  level: 0
  stage: ""
}
I1112 01:20:37.056398  3508 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test_4.prototxt
I1112 01:20:37.057377  3508 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1112 01:20:37.057453  3508 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1112 01:20:37.057862  3508 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu0"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I1112 01:20:37.058120  3508 layer_factory.hpp:77] Creating layer mnist
I1112 01:20:37.093171  3508 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I1112 01:20:37.093335  3508 net.cpp:84] Creating Layer mnist
I1112 01:20:37.113788  3508 net.cpp:380] mnist -> data
I1112 01:20:37.113984  3508 net.cpp:380] mnist -> label
I1112 01:20:37.114120  3508 data_layer.cpp:45] output data size: 64,1,28,28
I1112 01:20:37.116344  3508 net.cpp:122] Setting up mnist
I1112 01:20:37.116456  3508 net.cpp:129] Top shape: 64 1 28 28 (50176)
I1112 01:20:37.116511  3508 net.cpp:129] Top shape: 64 (64)
I1112 01:20:37.116624  3508 net.cpp:137] Memory required for data: 200960
I1112 01:20:37.116675  3508 layer_factory.hpp:77] Creating layer conv1
I1112 01:20:37.116792  3508 net.cpp:84] Creating Layer conv1
I1112 01:20:37.116840  3508 net.cpp:406] conv1 <- data
I1112 01:20:37.116966  3508 net.cpp:380] conv1 -> conv1
I1112 01:20:37.117257  3508 net.cpp:122] Setting up conv1
I1112 01:20:37.117336  3508 net.cpp:129] Top shape: 64 20 24 24 (737280)
I1112 01:20:37.117374  3508 net.cpp:137] Memory required for data: 3150080
I1112 01:20:37.117491  3508 layer_factory.hpp:77] Creating layer pool1
I1112 01:20:37.117588  3508 net.cpp:84] Creating Layer pool1
I1112 01:20:37.117632  3508 net.cpp:406] pool1 <- conv1
I1112 01:20:37.117686  3508 net.cpp:380] pool1 -> pool1
I1112 01:20:37.117790  3508 net.cpp:122] Setting up pool1
I1112 01:20:37.117852  3508 net.cpp:129] Top shape: 64 20 12 12 (184320)
I1112 01:20:37.117889  3508 net.cpp:137] Memory required for data: 3887360
I1112 01:20:37.117934  3508 layer_factory.hpp:77] Creating layer relu0
I1112 01:20:37.117980  3508 net.cpp:84] Creating Layer relu0
I1112 01:20:37.118026  3508 net.cpp:406] relu0 <- pool1
I1112 01:20:37.118084  3508 net.cpp:367] relu0 -> pool1 (in-place)
I1112 01:20:37.118140  3508 net.cpp:122] Setting up relu0
I1112 01:20:37.118201  3508 net.cpp:129] Top shape: 64 20 12 12 (184320)
I1112 01:20:37.118237  3508 net.cpp:137] Memory required for data: 4624640
I1112 01:20:37.118270  3508 layer_factory.hpp:77] Creating layer conv2
I1112 01:20:37.118338  3508 net.cpp:84] Creating Layer conv2
I1112 01:20:37.118377  3508 net.cpp:406] conv2 <- pool1
I1112 01:20:37.118438  3508 net.cpp:380] conv2 -> conv2
I1112 01:20:37.120345  3508 net.cpp:122] Setting up conv2
I1112 01:20:37.120453  3508 net.cpp:129] Top shape: 64 50 8 8 (204800)
I1112 01:20:37.120501  3508 net.cpp:137] Memory required for data: 5443840
I1112 01:20:37.120584  3508 layer_factory.hpp:77] Creating layer pool2
I1112 01:20:37.120654  3508 net.cpp:84] Creating Layer pool2
I1112 01:20:37.120702  3508 net.cpp:406] pool2 <- conv2
I1112 01:20:37.120771  3508 net.cpp:380] pool2 -> pool2
I1112 01:20:37.120853  3508 net.cpp:122] Setting up pool2
I1112 01:20:37.120903  3508 net.cpp:129] Top shape: 64 50 4 4 (51200)
I1112 01:20:37.120936  3508 net.cpp:137] Memory required for data: 5648640
I1112 01:20:37.120980  3508 layer_factory.hpp:77] Creating layer ip1
I1112 01:20:37.121039  3508 net.cpp:84] Creating Layer ip1
I1112 01:20:37.121078  3508 net.cpp:406] ip1 <- pool2
I1112 01:20:37.121372  3508 net.cpp:380] ip1 -> ip1
I1112 01:20:37.141543  3508 net.cpp:122] Setting up ip1
I1112 01:20:37.141659  3508 net.cpp:129] Top shape: 64 500 (32000)
I1112 01:20:37.141698  3508 net.cpp:137] Memory required for data: 5776640
I1112 01:20:37.141773  3508 layer_factory.hpp:77] Creating layer relu1
I1112 01:20:37.141824  3508 net.cpp:84] Creating Layer relu1
I1112 01:20:37.141857  3508 net.cpp:406] relu1 <- ip1
I1112 01:20:37.141908  3508 net.cpp:367] relu1 -> ip1 (in-place)
I1112 01:20:37.141981  3508 net.cpp:122] Setting up relu1
I1112 01:20:37.142020  3508 net.cpp:129] Top shape: 64 500 (32000)
I1112 01:20:37.142050  3508 net.cpp:137] Memory required for data: 5904640
I1112 01:20:37.142081  3508 layer_factory.hpp:77] Creating layer ip2
I1112 01:20:37.142133  3508 net.cpp:84] Creating Layer ip2
I1112 01:20:37.142166  3508 net.cpp:406] ip2 <- ip1
I1112 01:20:37.142220  3508 net.cpp:380] ip2 -> ip2
I1112 01:20:37.156981  3508 net.cpp:122] Setting up ip2
I1112 01:20:37.157095  3508 net.cpp:129] Top shape: 64 500 (32000)
I1112 01:20:37.157137  3508 net.cpp:137] Memory required for data: 6032640
I1112 01:20:37.157199  3508 layer_factory.hpp:77] Creating layer relu2
I1112 01:20:37.157258  3508 net.cpp:84] Creating Layer relu2
I1112 01:20:37.157299  3508 net.cpp:406] relu2 <- ip2
I1112 01:20:37.157351  3508 net.cpp:367] relu2 -> ip2 (in-place)
I1112 01:20:37.157409  3508 net.cpp:122] Setting up relu2
I1112 01:20:37.157455  3508 net.cpp:129] Top shape: 64 500 (32000)
I1112 01:20:37.157487  3508 net.cpp:137] Memory required for data: 6160640
I1112 01:20:37.157518  3508 layer_factory.hpp:77] Creating layer ip3
I1112 01:20:37.157570  3508 net.cpp:84] Creating Layer ip3
I1112 01:20:37.157605  3508 net.cpp:406] ip3 <- ip2
I1112 01:20:37.157657  3508 net.cpp:380] ip3 -> ip3
I1112 01:20:37.158109  3508 net.cpp:122] Setting up ip3
I1112 01:20:37.158215  3508 net.cpp:129] Top shape: 64 10 (640)
I1112 01:20:37.158251  3508 net.cpp:137] Memory required for data: 6163200
I1112 01:20:37.158318  3508 layer_factory.hpp:77] Creating layer loss
I1112 01:20:37.158371  3508 net.cpp:84] Creating Layer loss
I1112 01:20:37.158407  3508 net.cpp:406] loss <- ip3
I1112 01:20:37.158449  3508 net.cpp:406] loss <- label
I1112 01:20:37.158502  3508 net.cpp:380] loss -> loss
I1112 01:20:37.158581  3508 layer_factory.hpp:77] Creating layer loss
I1112 01:20:37.158680  3508 net.cpp:122] Setting up loss
I1112 01:20:37.158725  3508 net.cpp:129] Top shape: (1)
I1112 01:20:37.158754  3508 net.cpp:132]     with loss weight 1
I1112 01:20:37.158845  3508 net.cpp:137] Memory required for data: 6163204
I1112 01:20:37.158895  3508 net.cpp:198] loss needs backward computation.
I1112 01:20:37.158941  3508 net.cpp:198] ip3 needs backward computation.
I1112 01:20:37.158973  3508 net.cpp:198] relu2 needs backward computation.
I1112 01:20:37.159003  3508 net.cpp:198] ip2 needs backward computation.
I1112 01:20:37.159034  3508 net.cpp:198] relu1 needs backward computation.
I1112 01:20:37.159063  3508 net.cpp:198] ip1 needs backward computation.
I1112 01:20:37.159096  3508 net.cpp:198] pool2 needs backward computation.
I1112 01:20:37.159127  3508 net.cpp:198] conv2 needs backward computation.
I1112 01:20:37.159160  3508 net.cpp:198] relu0 needs backward computation.
I1112 01:20:37.159193  3508 net.cpp:198] pool1 needs backward computation.
I1112 01:20:37.159226  3508 net.cpp:198] conv1 needs backward computation.
I1112 01:20:37.159263  3508 net.cpp:200] mnist does not need backward computation.
I1112 01:20:37.159327  3508 net.cpp:242] This network produces output loss
I1112 01:20:37.159409  3508 net.cpp:255] Network initialization done.
I1112 01:20:37.160571  3508 solver.cpp:173] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test_4.prototxt
I1112 01:20:37.160758  3508 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1112 01:20:37.161476  3508 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu0"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I1112 01:20:37.162029  3508 layer_factory.hpp:77] Creating layer mnist
I1112 01:20:37.166985  3508 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I1112 01:20:37.167064  3508 net.cpp:84] Creating Layer mnist
I1112 01:20:37.167104  3508 net.cpp:380] mnist -> data
I1112 01:20:37.167135  3508 net.cpp:380] mnist -> label
I1112 01:20:37.167179  3508 data_layer.cpp:45] output data size: 100,1,28,28
I1112 01:20:37.167323  3508 net.cpp:122] Setting up mnist
I1112 01:20:37.167351  3508 net.cpp:129] Top shape: 100 1 28 28 (78400)
I1112 01:20:37.167366  3508 net.cpp:129] Top shape: 100 (100)
I1112 01:20:37.167376  3508 net.cpp:137] Memory required for data: 314000
I1112 01:20:37.167388  3508 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1112 01:20:37.167421  3508 net.cpp:84] Creating Layer label_mnist_1_split
I1112 01:20:37.167435  3508 net.cpp:406] label_mnist_1_split <- label
I1112 01:20:37.167453  3508 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I1112 01:20:37.167474  3508 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I1112 01:20:37.167500  3508 net.cpp:122] Setting up label_mnist_1_split
I1112 01:20:37.167515  3508 net.cpp:129] Top shape: 100 (100)
I1112 01:20:37.167529  3508 net.cpp:129] Top shape: 100 (100)
I1112 01:20:37.167539  3508 net.cpp:137] Memory required for data: 314800
I1112 01:20:37.167551  3508 layer_factory.hpp:77] Creating layer conv1
I1112 01:20:37.167577  3508 net.cpp:84] Creating Layer conv1
I1112 01:20:37.167596  3508 net.cpp:406] conv1 <- data
I1112 01:20:37.167615  3508 net.cpp:380] conv1 -> conv1
I1112 01:20:37.167693  3508 net.cpp:122] Setting up conv1
I1112 01:20:37.167712  3508 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I1112 01:20:37.167728  3508 net.cpp:137] Memory required for data: 4922800
I1112 01:20:37.167754  3508 layer_factory.hpp:77] Creating layer pool1
I1112 01:20:37.167778  3508 net.cpp:84] Creating Layer pool1
I1112 01:20:37.167790  3508 net.cpp:406] pool1 <- conv1
I1112 01:20:37.167809  3508 net.cpp:380] pool1 -> pool1
I1112 01:20:37.167835  3508 net.cpp:122] Setting up pool1
I1112 01:20:37.167855  3508 net.cpp:129] Top shape: 100 20 12 12 (288000)
I1112 01:20:37.167866  3508 net.cpp:137] Memory required for data: 6074800
I1112 01:20:37.167876  3508 layer_factory.hpp:77] Creating layer relu0
I1112 01:20:37.167893  3508 net.cpp:84] Creating Layer relu0
I1112 01:20:37.167904  3508 net.cpp:406] relu0 <- pool1
I1112 01:20:37.167918  3508 net.cpp:367] relu0 -> pool1 (in-place)
I1112 01:20:37.167935  3508 net.cpp:122] Setting up relu0
I1112 01:20:37.167949  3508 net.cpp:129] Top shape: 100 20 12 12 (288000)
I1112 01:20:37.167959  3508 net.cpp:137] Memory required for data: 7226800
I1112 01:20:37.167968  3508 layer_factory.hpp:77] Creating layer conv2
I1112 01:20:37.167995  3508 net.cpp:84] Creating Layer conv2
I1112 01:20:37.168007  3508 net.cpp:406] conv2 <- pool1
I1112 01:20:37.168051  3508 net.cpp:380] conv2 -> conv2
I1112 01:20:37.168846  3508 net.cpp:122] Setting up conv2
I1112 01:20:37.168911  3508 net.cpp:129] Top shape: 100 50 8 8 (320000)
I1112 01:20:37.168926  3508 net.cpp:137] Memory required for data: 8506800
I1112 01:20:37.168951  3508 layer_factory.hpp:77] Creating layer pool2
I1112 01:20:37.168972  3508 net.cpp:84] Creating Layer pool2
I1112 01:20:37.168984  3508 net.cpp:406] pool2 <- conv2
I1112 01:20:37.169003  3508 net.cpp:380] pool2 -> pool2
I1112 01:20:37.169031  3508 net.cpp:122] Setting up pool2
I1112 01:20:37.169050  3508 net.cpp:129] Top shape: 100 50 4 4 (80000)
I1112 01:20:37.169076  3508 net.cpp:137] Memory required for data: 8826800
I1112 01:20:37.169104  3508 layer_factory.hpp:77] Creating layer ip1
I1112 01:20:37.169127  3508 net.cpp:84] Creating Layer ip1
I1112 01:20:37.169143  3508 net.cpp:406] ip1 <- pool2
I1112 01:20:37.169163  3508 net.cpp:380] ip1 -> ip1
I1112 01:20:37.178084  3508 net.cpp:122] Setting up ip1
I1112 01:20:37.178133  3508 net.cpp:129] Top shape: 100 500 (50000)
I1112 01:20:37.178141  3508 net.cpp:137] Memory required for data: 9026800
I1112 01:20:37.178167  3508 layer_factory.hpp:77] Creating layer relu1
I1112 01:20:37.178187  3508 net.cpp:84] Creating Layer relu1
I1112 01:20:37.178197  3508 net.cpp:406] relu1 <- ip1
I1112 01:20:37.178208  3508 net.cpp:367] relu1 -> ip1 (in-place)
I1112 01:20:37.178223  3508 net.cpp:122] Setting up relu1
I1112 01:20:37.178233  3508 net.cpp:129] Top shape: 100 500 (50000)
I1112 01:20:37.178239  3508 net.cpp:137] Memory required for data: 9226800
I1112 01:20:37.178247  3508 layer_factory.hpp:77] Creating layer ip2
I1112 01:20:37.178259  3508 net.cpp:84] Creating Layer ip2
I1112 01:20:37.178267  3508 net.cpp:406] ip2 <- ip1
I1112 01:20:37.178280  3508 net.cpp:380] ip2 -> ip2
I1112 01:20:37.181457  3508 net.cpp:122] Setting up ip2
I1112 01:20:37.181469  3508 net.cpp:129] Top shape: 100 500 (50000)
I1112 01:20:37.181474  3508 net.cpp:137] Memory required for data: 9426800
I1112 01:20:37.181483  3508 layer_factory.hpp:77] Creating layer relu2
I1112 01:20:37.181494  3508 net.cpp:84] Creating Layer relu2
I1112 01:20:37.181500  3508 net.cpp:406] relu2 <- ip2
I1112 01:20:37.181509  3508 net.cpp:367] relu2 -> ip2 (in-place)
I1112 01:20:37.181516  3508 net.cpp:122] Setting up relu2
I1112 01:20:37.181524  3508 net.cpp:129] Top shape: 100 500 (50000)
I1112 01:20:37.181529  3508 net.cpp:137] Memory required for data: 9626800
I1112 01:20:37.181533  3508 layer_factory.hpp:77] Creating layer ip3
I1112 01:20:37.181543  3508 net.cpp:84] Creating Layer ip3
I1112 01:20:37.181550  3508 net.cpp:406] ip3 <- ip2
I1112 01:20:37.181557  3508 net.cpp:380] ip3 -> ip3
I1112 01:20:37.181618  3508 net.cpp:122] Setting up ip3
I1112 01:20:37.181627  3508 net.cpp:129] Top shape: 100 10 (1000)
I1112 01:20:37.181632  3508 net.cpp:137] Memory required for data: 9630800
I1112 01:20:37.181644  3508 layer_factory.hpp:77] Creating layer ip3_ip3_0_split
I1112 01:20:37.181653  3508 net.cpp:84] Creating Layer ip3_ip3_0_split
I1112 01:20:37.181659  3508 net.cpp:406] ip3_ip3_0_split <- ip3
I1112 01:20:37.181668  3508 net.cpp:380] ip3_ip3_0_split -> ip3_ip3_0_split_0
I1112 01:20:37.181677  3508 net.cpp:380] ip3_ip3_0_split -> ip3_ip3_0_split_1
I1112 01:20:37.181689  3508 net.cpp:122] Setting up ip3_ip3_0_split
I1112 01:20:37.181695  3508 net.cpp:129] Top shape: 100 10 (1000)
I1112 01:20:37.181701  3508 net.cpp:129] Top shape: 100 10 (1000)
I1112 01:20:37.181706  3508 net.cpp:137] Memory required for data: 9638800
I1112 01:20:37.181711  3508 layer_factory.hpp:77] Creating layer accuracy
I1112 01:20:37.181722  3508 net.cpp:84] Creating Layer accuracy
I1112 01:20:37.181728  3508 net.cpp:406] accuracy <- ip3_ip3_0_split_0
I1112 01:20:37.181735  3508 net.cpp:406] accuracy <- label_mnist_1_split_0
I1112 01:20:37.181744  3508 net.cpp:380] accuracy -> accuracy
I1112 01:20:37.181756  3508 net.cpp:122] Setting up accuracy
I1112 01:20:37.181762  3508 net.cpp:129] Top shape: (1)
I1112 01:20:37.181767  3508 net.cpp:137] Memory required for data: 9638804
I1112 01:20:37.181772  3508 layer_factory.hpp:77] Creating layer loss
I1112 01:20:37.181780  3508 net.cpp:84] Creating Layer loss
I1112 01:20:37.181787  3508 net.cpp:406] loss <- ip3_ip3_0_split_1
I1112 01:20:37.181793  3508 net.cpp:406] loss <- label_mnist_1_split_1
I1112 01:20:37.181802  3508 net.cpp:380] loss -> loss
I1112 01:20:37.181816  3508 layer_factory.hpp:77] Creating layer loss
I1112 01:20:37.181838  3508 net.cpp:122] Setting up loss
I1112 01:20:37.181845  3508 net.cpp:129] Top shape: (1)
I1112 01:20:37.181850  3508 net.cpp:132]     with loss weight 1
I1112 01:20:37.181864  3508 net.cpp:137] Memory required for data: 9638808
I1112 01:20:37.181879  3508 net.cpp:198] loss needs backward computation.
I1112 01:20:37.181895  3508 net.cpp:200] accuracy does not need backward computation.
I1112 01:20:37.181901  3508 net.cpp:198] ip3_ip3_0_split needs backward computation.
I1112 01:20:37.181907  3508 net.cpp:198] ip3 needs backward computation.
I1112 01:20:37.181913  3508 net.cpp:198] relu2 needs backward computation.
I1112 01:20:37.181918  3508 net.cpp:198] ip2 needs backward computation.
I1112 01:20:37.181924  3508 net.cpp:198] relu1 needs backward computation.
I1112 01:20:37.181931  3508 net.cpp:198] ip1 needs backward computation.
I1112 01:20:37.181937  3508 net.cpp:198] pool2 needs backward computation.
I1112 01:20:37.181943  3508 net.cpp:198] conv2 needs backward computation.
I1112 01:20:37.181949  3508 net.cpp:198] relu0 needs backward computation.
I1112 01:20:37.181955  3508 net.cpp:198] pool1 needs backward computation.
I1112 01:20:37.181960  3508 net.cpp:198] conv1 needs backward computation.
I1112 01:20:37.181967  3508 net.cpp:200] label_mnist_1_split does not need backward computation.
I1112 01:20:37.181974  3508 net.cpp:200] mnist does not need backward computation.
I1112 01:20:37.181979  3508 net.cpp:242] This network produces output accuracy
I1112 01:20:37.181985  3508 net.cpp:242] This network produces output loss
I1112 01:20:37.182004  3508 net.cpp:255] Network initialization done.
I1112 01:20:37.182071  3508 solver.cpp:56] Solver scaffolding done.
I1112 01:20:37.182116  3508 caffe.cpp:248] Starting Optimization
I1112 01:20:37.182123  3508 solver.cpp:273] Solving LeNet
I1112 01:20:37.182128  3508 solver.cpp:274] Learning Rate Policy: inv
I1112 01:20:37.183437  3508 solver.cpp:331] Iteration 0, Testing net (#0)
I1112 01:20:50.587630  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:20:51.174818  3508 solver.cpp:398]     Test net output #0: accuracy = 0.1339
I1112 01:20:51.174898  3508 solver.cpp:398]     Test net output #1: loss = 2.31358 (* 1 = 2.31358 loss)
I1112 01:20:51.369642  3508 solver.cpp:219] Iteration 0 (-1.4013e-45 iter/s, 14.187s/100 iters), loss = 2.3317
I1112 01:20:51.369683  3508 solver.cpp:238]     Train net output #0: loss = 2.3317 (* 1 = 2.3317 loss)
I1112 01:20:51.369731  3508 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1112 01:21:00.554963  3508 solver.cpp:219] Iteration 100 (10.8873 iter/s, 9.185s/100 iters), loss = 0.159738
I1112 01:21:00.555033  3508 solver.cpp:238]     Train net output #0: loss = 0.159738 (* 1 = 0.159738 loss)
I1112 01:21:00.555045  3508 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I1112 01:21:09.950352  3508 solver.cpp:219] Iteration 200 (10.644 iter/s, 9.395s/100 iters), loss = 0.152568
I1112 01:21:09.953361  3508 solver.cpp:238]     Train net output #0: loss = 0.152568 (* 1 = 0.152568 loss)
I1112 01:21:09.953377  3508 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I1112 01:21:19.385399  3508 solver.cpp:219] Iteration 300 (10.6022 iter/s, 9.432s/100 iters), loss = 0.153522
I1112 01:21:19.385447  3508 solver.cpp:238]     Train net output #0: loss = 0.153522 (* 1 = 0.153522 loss)
I1112 01:21:19.385473  3508 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I1112 01:21:28.243986  3508 solver.cpp:219] Iteration 400 (11.2892 iter/s, 8.858s/100 iters), loss = 0.0545016
I1112 01:21:28.244035  3508 solver.cpp:238]     Train net output #0: loss = 0.0545016 (* 1 = 0.0545016 loss)
I1112 01:21:28.244063  3508 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I1112 01:21:35.764799  3508 blocking_queue.cpp:49] Waiting for data
I1112 01:21:37.405009  3508 solver.cpp:331] Iteration 500, Testing net (#0)
I1112 01:21:43.137202  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:21:43.418776  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9759
I1112 01:21:43.418833  3508 solver.cpp:398]     Test net output #1: loss = 0.0727164 (* 1 = 0.0727164 loss)
I1112 01:21:43.519521  3508 solver.cpp:219] Iteration 500 (6.54665 iter/s, 15.275s/100 iters), loss = 0.0586386
I1112 01:21:43.519572  3508 solver.cpp:238]     Train net output #0: loss = 0.0586386 (* 1 = 0.0586386 loss)
I1112 01:21:43.519594  3508 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I1112 01:21:55.250491  3508 solver.cpp:219] Iteration 600 (8.52515 iter/s, 11.73s/100 iters), loss = 0.0899309
I1112 01:21:55.250546  3508 solver.cpp:238]     Train net output #0: loss = 0.0899309 (* 1 = 0.0899309 loss)
I1112 01:21:55.250558  3508 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I1112 01:22:07.477692  3508 solver.cpp:219] Iteration 700 (8.17862 iter/s, 12.227s/100 iters), loss = 0.148578
I1112 01:22:07.477742  3508 solver.cpp:238]     Train net output #0: loss = 0.148578 (* 1 = 0.148578 loss)
I1112 01:22:07.477753  3508 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I1112 01:22:17.490223  3508 solver.cpp:219] Iteration 800 (9.98801 iter/s, 10.012s/100 iters), loss = 0.199321
I1112 01:22:17.490353  3508 solver.cpp:238]     Train net output #0: loss = 0.199321 (* 1 = 0.199321 loss)
I1112 01:22:17.490365  3508 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I1112 01:22:27.773082  3508 solver.cpp:219] Iteration 900 (9.72573 iter/s, 10.282s/100 iters), loss = 0.142058
I1112 01:22:27.773129  3508 solver.cpp:238]     Train net output #0: loss = 0.142058 (* 1 = 0.142058 loss)
I1112 01:22:27.773162  3508 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I1112 01:22:31.151800  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:22:38.178494  3508 solver.cpp:331] Iteration 1000, Testing net (#0)
I1112 01:22:44.285017  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:22:44.510224  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9835
I1112 01:22:44.510279  3508 solver.cpp:398]     Test net output #1: loss = 0.0505854 (* 1 = 0.0505854 loss)
I1112 01:22:44.596923  3508 solver.cpp:219] Iteration 1000 (5.94424 iter/s, 16.823s/100 iters), loss = 0.113749
I1112 01:22:44.596962  3508 solver.cpp:238]     Train net output #0: loss = 0.113748 (* 1 = 0.113748 loss)
I1112 01:22:44.596992  3508 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I1112 01:22:55.318862  3508 solver.cpp:219] Iteration 1100 (9.32749 iter/s, 10.721s/100 iters), loss = 0.00516998
I1112 01:22:55.318992  3508 solver.cpp:238]     Train net output #0: loss = 0.00516989 (* 1 = 0.00516989 loss)
I1112 01:22:55.319008  3508 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I1112 01:23:06.951349  3508 solver.cpp:219] Iteration 1200 (8.59697 iter/s, 11.632s/100 iters), loss = 0.0398095
I1112 01:23:06.951443  3508 solver.cpp:238]     Train net output #0: loss = 0.0398094 (* 1 = 0.0398094 loss)
I1112 01:23:06.951458  3508 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I1112 01:23:17.851986  3508 solver.cpp:219] Iteration 1300 (9.17431 iter/s, 10.9s/100 iters), loss = 0.0154837
I1112 01:23:17.852092  3508 solver.cpp:238]     Train net output #0: loss = 0.0154836 (* 1 = 0.0154836 loss)
I1112 01:23:17.852118  3508 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I1112 01:23:29.272961  3508 solver.cpp:219] Iteration 1400 (8.75657 iter/s, 11.42s/100 iters), loss = 0.00368901
I1112 01:23:29.273216  3508 solver.cpp:238]     Train net output #0: loss = 0.00368893 (* 1 = 0.00368893 loss)
I1112 01:23:29.273243  3508 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I1112 01:23:42.220186  3508 solver.cpp:331] Iteration 1500, Testing net (#0)
I1112 01:23:48.587941  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:23:48.867657  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9858
I1112 01:23:48.867704  3508 solver.cpp:398]     Test net output #1: loss = 0.0413595 (* 1 = 0.0413595 loss)
I1112 01:23:48.970456  3508 solver.cpp:219] Iteration 1500 (5.07692 iter/s, 19.697s/100 iters), loss = 0.0613491
I1112 01:23:48.970494  3508 solver.cpp:238]     Train net output #0: loss = 0.0613491 (* 1 = 0.0613491 loss)
I1112 01:23:48.970525  3508 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I1112 01:23:58.216701  3508 solver.cpp:219] Iteration 1600 (10.8155 iter/s, 9.246s/100 iters), loss = 0.0983468
I1112 01:23:58.216747  3508 solver.cpp:238]     Train net output #0: loss = 0.0983468 (* 1 = 0.0983468 loss)
I1112 01:23:58.216785  3508 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I1112 01:24:08.432061  3508 solver.cpp:219] Iteration 1700 (9.78953 iter/s, 10.215s/100 iters), loss = 0.0416358
I1112 01:24:08.432328  3508 solver.cpp:238]     Train net output #0: loss = 0.0416357 (* 1 = 0.0416357 loss)
I1112 01:24:08.432343  3508 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I1112 01:24:20.102285  3508 solver.cpp:219] Iteration 1800 (8.56971 iter/s, 11.669s/100 iters), loss = 0.0117275
I1112 01:24:20.102336  3508 solver.cpp:238]     Train net output #0: loss = 0.0117275 (* 1 = 0.0117275 loss)
I1112 01:24:20.102360  3508 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I1112 01:24:27.493609  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:24:30.752954  3508 solver.cpp:219] Iteration 1900 (9.38967 iter/s, 10.65s/100 iters), loss = 0.0961663
I1112 01:24:30.753001  3508 solver.cpp:238]     Train net output #0: loss = 0.0961662 (* 1 = 0.0961662 loss)
I1112 01:24:30.753011  3508 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I1112 01:24:40.956071  3508 solver.cpp:331] Iteration 2000, Testing net (#0)
I1112 01:24:46.472945  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:24:46.705795  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9863
I1112 01:24:46.705839  3508 solver.cpp:398]     Test net output #1: loss = 0.0450244 (* 1 = 0.0450244 loss)
I1112 01:24:46.799479  3508 solver.cpp:219] Iteration 2000 (6.23208 iter/s, 16.046s/100 iters), loss = 0.00763603
I1112 01:24:46.799536  3508 solver.cpp:238]     Train net output #0: loss = 0.007636 (* 1 = 0.007636 loss)
I1112 01:24:46.799551  3508 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I1112 01:24:57.268682  3508 solver.cpp:219] Iteration 2100 (9.55201 iter/s, 10.469s/100 iters), loss = 0.0112391
I1112 01:24:57.268730  3508 solver.cpp:238]     Train net output #0: loss = 0.011239 (* 1 = 0.011239 loss)
I1112 01:24:57.268761  3508 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I1112 01:25:06.738401  3508 solver.cpp:219] Iteration 2200 (10.5608 iter/s, 9.469s/100 iters), loss = 0.0111939
I1112 01:25:06.738448  3508 solver.cpp:238]     Train net output #0: loss = 0.0111939 (* 1 = 0.0111939 loss)
I1112 01:25:06.738461  3508 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I1112 01:25:16.630738  3508 solver.cpp:219] Iteration 2300 (10.1092 iter/s, 9.892s/100 iters), loss = 0.0650938
I1112 01:25:16.630928  3508 solver.cpp:238]     Train net output #0: loss = 0.0650938 (* 1 = 0.0650938 loss)
I1112 01:25:16.630939  3508 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I1112 01:25:26.260591  3508 solver.cpp:219] Iteration 2400 (10.3853 iter/s, 9.629s/100 iters), loss = 0.00898297
I1112 01:25:26.260677  3508 solver.cpp:238]     Train net output #0: loss = 0.00898295 (* 1 = 0.00898295 loss)
I1112 01:25:26.260697  3508 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I1112 01:25:36.896680  3508 solver.cpp:331] Iteration 2500, Testing net (#0)
I1112 01:25:44.049078  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:25:44.296808  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9865
I1112 01:25:44.296854  3508 solver.cpp:398]     Test net output #1: loss = 0.0419741 (* 1 = 0.0419741 loss)
I1112 01:25:44.388664  3508 solver.cpp:219] Iteration 2500 (5.51663 iter/s, 18.127s/100 iters), loss = 0.0453109
I1112 01:25:44.388711  3508 solver.cpp:238]     Train net output #0: loss = 0.0453108 (* 1 = 0.0453108 loss)
I1112 01:25:44.388721  3508 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I1112 01:25:55.369313  3508 solver.cpp:219] Iteration 2600 (9.10747 iter/s, 10.98s/100 iters), loss = 0.0555509
I1112 01:25:55.369395  3508 solver.cpp:238]     Train net output #0: loss = 0.0555509 (* 1 = 0.0555509 loss)
I1112 01:25:55.369405  3508 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I1112 01:26:06.748939  3508 solver.cpp:219] Iteration 2700 (8.78812 iter/s, 11.379s/100 iters), loss = 0.0411795
I1112 01:26:06.748999  3508 solver.cpp:238]     Train net output #0: loss = 0.0411795 (* 1 = 0.0411795 loss)
I1112 01:26:06.749019  3508 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I1112 01:26:17.413725  3508 solver.cpp:219] Iteration 2800 (9.37735 iter/s, 10.664s/100 iters), loss = 0.00338692
I1112 01:26:17.413770  3508 solver.cpp:238]     Train net output #0: loss = 0.00338689 (* 1 = 0.00338689 loss)
I1112 01:26:17.413796  3508 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I1112 01:26:18.207579  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:26:27.129019  3508 solver.cpp:219] Iteration 2900 (10.2934 iter/s, 9.715s/100 iters), loss = 0.0128438
I1112 01:26:27.129277  3508 solver.cpp:238]     Train net output #0: loss = 0.0128438 (* 1 = 0.0128438 loss)
I1112 01:26:27.129292  3508 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I1112 01:26:36.382649  3508 solver.cpp:331] Iteration 3000, Testing net (#0)
I1112 01:26:42.180968  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:26:42.474287  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9884
I1112 01:26:42.474347  3508 solver.cpp:398]     Test net output #1: loss = 0.0345392 (* 1 = 0.0345392 loss)
I1112 01:26:42.590039  3508 solver.cpp:219] Iteration 3000 (6.46831 iter/s, 15.46s/100 iters), loss = 0.0128669
I1112 01:26:42.590095  3508 solver.cpp:238]     Train net output #0: loss = 0.0128669 (* 1 = 0.0128669 loss)
I1112 01:26:42.590109  3508 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I1112 01:26:53.765458  3508 solver.cpp:219] Iteration 3100 (8.94855 iter/s, 11.175s/100 iters), loss = 0.00939584
I1112 01:26:53.765516  3508 solver.cpp:238]     Train net output #0: loss = 0.00939582 (* 1 = 0.00939582 loss)
I1112 01:26:53.765529  3508 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I1112 01:27:03.438635  3508 solver.cpp:219] Iteration 3200 (10.3381 iter/s, 9.673s/100 iters), loss = 0.00497135
I1112 01:27:03.438763  3508 solver.cpp:238]     Train net output #0: loss = 0.00497134 (* 1 = 0.00497134 loss)
I1112 01:27:03.438794  3508 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I1112 01:27:13.766567  3508 solver.cpp:219] Iteration 3300 (9.68335 iter/s, 10.327s/100 iters), loss = 0.0166438
I1112 01:27:13.766618  3508 solver.cpp:238]     Train net output #0: loss = 0.0166437 (* 1 = 0.0166437 loss)
I1112 01:27:13.766628  3508 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I1112 01:27:23.810900  3508 solver.cpp:219] Iteration 3400 (9.95619 iter/s, 10.044s/100 iters), loss = 0.0109821
I1112 01:27:23.810946  3508 solver.cpp:238]     Train net output #0: loss = 0.0109821 (* 1 = 0.0109821 loss)
I1112 01:27:23.810977  3508 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I1112 01:27:34.447744  3508 solver.cpp:331] Iteration 3500, Testing net (#0)
I1112 01:27:40.113106  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:27:40.337363  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9851
I1112 01:27:40.337407  3508 solver.cpp:398]     Test net output #1: loss = 0.0429097 (* 1 = 0.0429097 loss)
I1112 01:27:40.432226  3508 solver.cpp:219] Iteration 3500 (6.01649 iter/s, 16.621s/100 iters), loss = 0.00461894
I1112 01:27:40.432271  3508 solver.cpp:238]     Train net output #0: loss = 0.00461889 (* 1 = 0.00461889 loss)
I1112 01:27:40.432297  3508 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I1112 01:27:49.922507  3508 solver.cpp:219] Iteration 3600 (10.5374 iter/s, 9.49s/100 iters), loss = 0.0317499
I1112 01:27:49.922566  3508 solver.cpp:238]     Train net output #0: loss = 0.0317498 (* 1 = 0.0317498 loss)
I1112 01:27:49.922574  3508 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I1112 01:28:00.071488  3508 solver.cpp:219] Iteration 3700 (9.85416 iter/s, 10.148s/100 iters), loss = 0.0163341
I1112 01:28:00.071563  3508 solver.cpp:238]     Train net output #0: loss = 0.016334 (* 1 = 0.016334 loss)
I1112 01:28:00.071573  3508 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I1112 01:28:04.485657  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:28:10.329910  3508 solver.cpp:219] Iteration 3800 (9.74849 iter/s, 10.258s/100 iters), loss = 0.00811942
I1112 01:28:10.329963  3508 solver.cpp:238]     Train net output #0: loss = 0.00811938 (* 1 = 0.00811938 loss)
I1112 01:28:10.329993  3508 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I1112 01:28:20.874270  3508 solver.cpp:219] Iteration 3900 (9.48407 iter/s, 10.544s/100 iters), loss = 0.0303908
I1112 01:28:20.874389  3508 solver.cpp:238]     Train net output #0: loss = 0.0303907 (* 1 = 0.0303907 loss)
I1112 01:28:20.874419  3508 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I1112 01:28:32.662402  3508 solver.cpp:331] Iteration 4000, Testing net (#0)
I1112 01:28:38.760975  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:28:39.015622  3508 solver.cpp:398]     Test net output #0: accuracy = 0.992
I1112 01:28:39.015668  3508 solver.cpp:398]     Test net output #1: loss = 0.0242238 (* 1 = 0.0242238 loss)
I1112 01:28:39.123364  3508 solver.cpp:219] Iteration 4000 (5.48005 iter/s, 18.248s/100 iters), loss = 0.0142512
I1112 01:28:39.123471  3508 solver.cpp:238]     Train net output #0: loss = 0.0142512 (* 1 = 0.0142512 loss)
I1112 01:28:39.123495  3508 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I1112 01:28:49.515161  3508 solver.cpp:219] Iteration 4100 (9.62371 iter/s, 10.391s/100 iters), loss = 0.00792175
I1112 01:28:49.515210  3508 solver.cpp:238]     Train net output #0: loss = 0.0079217 (* 1 = 0.0079217 loss)
I1112 01:28:49.515239  3508 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I1112 01:28:59.632108  3508 solver.cpp:219] Iteration 4200 (9.88533 iter/s, 10.116s/100 iters), loss = 0.00622734
I1112 01:28:59.632151  3508 solver.cpp:238]     Train net output #0: loss = 0.00622731 (* 1 = 0.00622731 loss)
I1112 01:28:59.632179  3508 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I1112 01:29:10.303735  3508 solver.cpp:219] Iteration 4300 (9.37119 iter/s, 10.671s/100 iters), loss = 0.0264423
I1112 01:29:10.304261  3508 solver.cpp:238]     Train net output #0: loss = 0.0264423 (* 1 = 0.0264423 loss)
I1112 01:29:10.304302  3508 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I1112 01:29:20.797117  3508 solver.cpp:219] Iteration 4400 (9.53107 iter/s, 10.492s/100 iters), loss = 0.0144426
I1112 01:29:20.797173  3508 solver.cpp:238]     Train net output #0: loss = 0.0144426 (* 1 = 0.0144426 loss)
I1112 01:29:20.797186  3508 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I1112 01:29:31.573755  3508 solver.cpp:331] Iteration 4500, Testing net (#0)
I1112 01:29:37.746773  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:29:37.996809  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9893
I1112 01:29:37.996872  3508 solver.cpp:398]     Test net output #1: loss = 0.0327105 (* 1 = 0.0327105 loss)
I1112 01:29:38.134068  3508 solver.cpp:219] Iteration 4500 (5.76834 iter/s, 17.336s/100 iters), loss = 0.00966782
I1112 01:29:38.134174  3508 solver.cpp:238]     Train net output #0: loss = 0.00966776 (* 1 = 0.00966776 loss)
I1112 01:29:38.134199  3508 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I1112 01:29:48.213567  3508 solver.cpp:219] Iteration 4600 (9.92162 iter/s, 10.079s/100 iters), loss = 0.00920007
I1112 01:29:48.213682  3508 solver.cpp:238]     Train net output #0: loss = 0.00920003 (* 1 = 0.00920003 loss)
I1112 01:29:48.213709  3508 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I1112 01:29:55.797477  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:29:57.333461  3508 solver.cpp:219] Iteration 4700 (10.9661 iter/s, 9.119s/100 iters), loss = 0.00492484
I1112 01:29:57.333525  3508 solver.cpp:238]     Train net output #0: loss = 0.00492479 (* 1 = 0.00492479 loss)
I1112 01:29:57.333535  3508 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I1112 01:30:06.470079  3508 solver.cpp:219] Iteration 4800 (10.9457 iter/s, 9.136s/100 iters), loss = 0.0110871
I1112 01:30:06.470126  3508 solver.cpp:238]     Train net output #0: loss = 0.011087 (* 1 = 0.011087 loss)
I1112 01:30:06.470166  3508 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I1112 01:30:16.657364  3508 solver.cpp:219] Iteration 4900 (9.81643 iter/s, 10.187s/100 iters), loss = 0.00742586
I1112 01:30:16.657414  3508 solver.cpp:238]     Train net output #0: loss = 0.00742579 (* 1 = 0.00742579 loss)
I1112 01:30:16.657423  3508 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I1112 01:30:26.535841  3508 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I1112 01:30:26.910444  3508 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I1112 01:30:26.935072  3508 solver.cpp:331] Iteration 5000, Testing net (#0)
I1112 01:30:32.686332  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:30:32.926717  3508 solver.cpp:398]     Test net output #0: accuracy = 0.992
I1112 01:30:32.926786  3508 solver.cpp:398]     Test net output #1: loss = 0.0254904 (* 1 = 0.0254904 loss)
I1112 01:30:33.028923  3508 solver.cpp:219] Iteration 5000 (6.10836 iter/s, 16.371s/100 iters), loss = 0.0171498
I1112 01:30:33.028967  3508 solver.cpp:238]     Train net output #0: loss = 0.0171497 (* 1 = 0.0171497 loss)
I1112 01:30:33.028995  3508 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I1112 01:30:42.878989  3508 solver.cpp:219] Iteration 5100 (10.1523 iter/s, 9.85s/100 iters), loss = 0.0244064
I1112 01:30:42.879036  3508 solver.cpp:238]     Train net output #0: loss = 0.0244063 (* 1 = 0.0244063 loss)
I1112 01:30:42.879076  3508 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I1112 01:30:52.721448  3508 solver.cpp:219] Iteration 5200 (10.1605 iter/s, 9.842s/100 iters), loss = 0.00978636
I1112 01:30:52.721509  3508 solver.cpp:238]     Train net output #0: loss = 0.00978629 (* 1 = 0.00978629 loss)
I1112 01:30:52.721519  3508 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I1112 01:31:02.558560  3508 solver.cpp:219] Iteration 5300 (10.1657 iter/s, 9.837s/100 iters), loss = 0.00103602
I1112 01:31:02.558770  3508 solver.cpp:238]     Train net output #0: loss = 0.00103595 (* 1 = 0.00103595 loss)
I1112 01:31:02.558794  3508 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I1112 01:31:12.326292  3508 solver.cpp:219] Iteration 5400 (10.2386 iter/s, 9.767s/100 iters), loss = 0.00862755
I1112 01:31:12.326357  3508 solver.cpp:238]     Train net output #0: loss = 0.00862748 (* 1 = 0.00862748 loss)
I1112 01:31:12.326366  3508 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I1112 01:31:22.046351  3508 solver.cpp:331] Iteration 5500, Testing net (#0)
I1112 01:31:27.819231  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:31:28.058702  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9904
I1112 01:31:28.058748  3508 solver.cpp:398]     Test net output #1: loss = 0.0283456 (* 1 = 0.0283456 loss)
I1112 01:31:28.152314  3508 solver.cpp:219] Iteration 5500 (6.31912 iter/s, 15.825s/100 iters), loss = 0.00317948
I1112 01:31:28.152377  3508 solver.cpp:238]     Train net output #0: loss = 0.00317941 (* 1 = 0.00317941 loss)
I1112 01:31:28.152387  3508 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I1112 01:31:38.033650  3508 solver.cpp:219] Iteration 5600 (10.1204 iter/s, 9.881s/100 iters), loss = 0.000535513
I1112 01:31:38.033885  3508 solver.cpp:238]     Train net output #0: loss = 0.00053545 (* 1 = 0.00053545 loss)
I1112 01:31:38.033897  3508 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I1112 01:31:40.018996  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:31:48.039083  3508 solver.cpp:219] Iteration 5700 (9.995 iter/s, 10.005s/100 iters), loss = 0.00459758
I1112 01:31:48.039135  3508 solver.cpp:238]     Train net output #0: loss = 0.00459751 (* 1 = 0.00459751 loss)
I1112 01:31:48.039145  3508 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I1112 01:31:58.498741  3508 solver.cpp:219] Iteration 5800 (9.56114 iter/s, 10.459s/100 iters), loss = 0.0257141
I1112 01:31:58.498786  3508 solver.cpp:238]     Train net output #0: loss = 0.025714 (* 1 = 0.025714 loss)
I1112 01:31:58.498813  3508 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I1112 01:32:08.753707  3508 solver.cpp:219] Iteration 5900 (9.75229 iter/s, 10.254s/100 iters), loss = 0.00367942
I1112 01:32:08.753986  3508 solver.cpp:238]     Train net output #0: loss = 0.00367934 (* 1 = 0.00367934 loss)
I1112 01:32:08.753999  3508 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I1112 01:32:18.208024  3508 solver.cpp:331] Iteration 6000, Testing net (#0)
I1112 01:32:24.146984  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:32:24.409603  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9923
I1112 01:32:24.409656  3508 solver.cpp:398]     Test net output #1: loss = 0.0232082 (* 1 = 0.0232082 loss)
I1112 01:32:24.518399  3508 solver.cpp:219] Iteration 6000 (6.34357 iter/s, 15.764s/100 iters), loss = 0.0034688
I1112 01:32:24.518450  3508 solver.cpp:238]     Train net output #0: loss = 0.00346871 (* 1 = 0.00346871 loss)
I1112 01:32:24.518458  3508 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I1112 01:32:34.480334  3508 solver.cpp:219] Iteration 6100 (10.0392 iter/s, 9.961s/100 iters), loss = 0.00215862
I1112 01:32:34.480381  3508 solver.cpp:238]     Train net output #0: loss = 0.00215853 (* 1 = 0.00215853 loss)
I1112 01:32:34.480408  3508 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I1112 01:32:44.432991  3508 solver.cpp:219] Iteration 6200 (10.0482 iter/s, 9.952s/100 iters), loss = 0.00688888
I1112 01:32:44.433230  3508 solver.cpp:238]     Train net output #0: loss = 0.00688879 (* 1 = 0.00688879 loss)
I1112 01:32:44.433243  3508 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I1112 01:32:54.467280  3508 solver.cpp:219] Iteration 6300 (9.96611 iter/s, 10.034s/100 iters), loss = 0.0106999
I1112 01:32:54.467327  3508 solver.cpp:238]     Train net output #0: loss = 0.0106998 (* 1 = 0.0106998 loss)
I1112 01:32:54.467355  3508 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I1112 01:33:04.447752  3508 solver.cpp:219] Iteration 6400 (10.02 iter/s, 9.98s/100 iters), loss = 0.00455442
I1112 01:33:04.447824  3508 solver.cpp:238]     Train net output #0: loss = 0.00455432 (* 1 = 0.00455432 loss)
I1112 01:33:04.447839  3508 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I1112 01:33:14.315838  3508 solver.cpp:331] Iteration 6500, Testing net (#0)
I1112 01:33:20.164783  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:33:20.406205  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9919
I1112 01:33:20.406262  3508 solver.cpp:398]     Test net output #1: loss = 0.0262494 (* 1 = 0.0262494 loss)
I1112 01:33:20.502781  3508 solver.cpp:219] Iteration 6500 (6.22898 iter/s, 16.054s/100 iters), loss = 0.0072866
I1112 01:33:20.502840  3508 solver.cpp:238]     Train net output #0: loss = 0.0072865 (* 1 = 0.0072865 loss)
I1112 01:33:20.502849  3508 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I1112 01:33:26.270813  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:33:30.472431  3508 solver.cpp:219] Iteration 6600 (10.0311 iter/s, 9.969s/100 iters), loss = 0.0151209
I1112 01:33:30.472496  3508 solver.cpp:238]     Train net output #0: loss = 0.0151208 (* 1 = 0.0151208 loss)
I1112 01:33:30.472506  3508 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I1112 01:33:40.462507  3508 solver.cpp:219] Iteration 6700 (10.01 iter/s, 9.99s/100 iters), loss = 0.00574008
I1112 01:33:40.462571  3508 solver.cpp:238]     Train net output #0: loss = 0.00573998 (* 1 = 0.00573998 loss)
I1112 01:33:40.462584  3508 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I1112 01:33:50.449399  3508 solver.cpp:219] Iteration 6800 (10.014 iter/s, 9.986s/100 iters), loss = 0.00188487
I1112 01:33:50.449491  3508 solver.cpp:238]     Train net output #0: loss = 0.00188477 (* 1 = 0.00188477 loss)
I1112 01:33:50.449501  3508 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I1112 01:34:00.513545  3508 solver.cpp:219] Iteration 6900 (9.93641 iter/s, 10.064s/100 iters), loss = 0.0040084
I1112 01:34:00.513600  3508 solver.cpp:238]     Train net output #0: loss = 0.00400831 (* 1 = 0.00400831 loss)
I1112 01:34:00.513613  3508 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I1112 01:34:10.467643  3508 solver.cpp:331] Iteration 7000, Testing net (#0)
I1112 01:34:16.335427  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:34:16.575250  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9914
I1112 01:34:16.575310  3508 solver.cpp:398]     Test net output #1: loss = 0.0255346 (* 1 = 0.0255346 loss)
I1112 01:34:16.671020  3508 solver.cpp:219] Iteration 7000 (6.18927 iter/s, 16.157s/100 iters), loss = 0.00850638
I1112 01:34:16.671064  3508 solver.cpp:238]     Train net output #0: loss = 0.00850628 (* 1 = 0.00850628 loss)
I1112 01:34:16.671092  3508 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I1112 01:34:26.721282  3508 solver.cpp:219] Iteration 7100 (9.95025 iter/s, 10.05s/100 iters), loss = 0.0100671
I1112 01:34:26.721513  3508 solver.cpp:238]     Train net output #0: loss = 0.010067 (* 1 = 0.010067 loss)
I1112 01:34:26.721534  3508 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I1112 01:34:36.746647  3508 solver.cpp:219] Iteration 7200 (9.97506 iter/s, 10.025s/100 iters), loss = 0.00876249
I1112 01:34:36.746698  3508 solver.cpp:238]     Train net output #0: loss = 0.00876239 (* 1 = 0.00876239 loss)
I1112 01:34:36.746728  3508 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I1112 01:34:46.755954  3508 solver.cpp:219] Iteration 7300 (9.99101 iter/s, 10.009s/100 iters), loss = 0.0158515
I1112 01:34:46.756019  3508 solver.cpp:238]     Train net output #0: loss = 0.0158514 (* 1 = 0.0158514 loss)
I1112 01:34:46.756034  3508 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I1112 01:34:56.664062  3508 solver.cpp:219] Iteration 7400 (10.0929 iter/s, 9.908s/100 iters), loss = 0.00392533
I1112 01:34:56.664108  3508 solver.cpp:238]     Train net output #0: loss = 0.00392525 (* 1 = 0.00392525 loss)
I1112 01:34:56.664130  3508 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I1112 01:35:06.290022  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:35:06.704783  3508 solver.cpp:331] Iteration 7500, Testing net (#0)
I1112 01:35:12.070392  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:35:12.294724  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9908
I1112 01:35:12.294770  3508 solver.cpp:398]     Test net output #1: loss = 0.0292643 (* 1 = 0.0292643 loss)
I1112 01:35:12.381621  3508 solver.cpp:219] Iteration 7500 (6.36254 iter/s, 15.717s/100 iters), loss = 0.000798028
I1112 01:35:12.381662  3508 solver.cpp:238]     Train net output #0: loss = 0.000797959 (* 1 = 0.000797959 loss)
I1112 01:35:12.381688  3508 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I1112 01:35:21.451532  3508 solver.cpp:219] Iteration 7600 (11.0266 iter/s, 9.069s/100 iters), loss = 0.00729312
I1112 01:35:21.451584  3508 solver.cpp:238]     Train net output #0: loss = 0.00729305 (* 1 = 0.00729305 loss)
I1112 01:35:21.451596  3508 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I1112 01:35:31.374939  3508 solver.cpp:219] Iteration 7700 (10.0776 iter/s, 9.923s/100 iters), loss = 0.0581825
I1112 01:35:31.374984  3508 solver.cpp:238]     Train net output #0: loss = 0.0581824 (* 1 = 0.0581824 loss)
I1112 01:35:31.375010  3508 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I1112 01:35:41.315789  3508 solver.cpp:219] Iteration 7800 (10.0604 iter/s, 9.94s/100 iters), loss = 0.0025233
I1112 01:35:41.315850  3508 solver.cpp:238]     Train net output #0: loss = 0.00252323 (* 1 = 0.00252323 loss)
I1112 01:35:41.315860  3508 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I1112 01:35:51.380188  3508 solver.cpp:219] Iteration 7900 (9.93641 iter/s, 10.064s/100 iters), loss = 0.00276479
I1112 01:35:51.380235  3508 solver.cpp:238]     Train net output #0: loss = 0.00276472 (* 1 = 0.00276472 loss)
I1112 01:35:51.380247  3508 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I1112 01:36:01.076638  3508 solver.cpp:331] Iteration 8000, Testing net (#0)
I1112 01:36:06.817529  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:36:07.056862  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9925
I1112 01:36:07.056936  3508 solver.cpp:398]     Test net output #1: loss = 0.0229719 (* 1 = 0.0229719 loss)
I1112 01:36:07.152281  3508 solver.cpp:219] Iteration 8000 (6.34035 iter/s, 15.772s/100 iters), loss = 0.00156377
I1112 01:36:07.152321  3508 solver.cpp:238]     Train net output #0: loss = 0.0015637 (* 1 = 0.0015637 loss)
I1112 01:36:07.152348  3508 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I1112 01:36:16.975967  3508 solver.cpp:219] Iteration 8100 (10.1802 iter/s, 9.823s/100 iters), loss = 0.0115728
I1112 01:36:16.976239  3508 solver.cpp:238]     Train net output #0: loss = 0.0115728 (* 1 = 0.0115728 loss)
I1112 01:36:16.976253  3508 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I1112 01:36:26.811749  3508 solver.cpp:219] Iteration 8200 (10.1678 iter/s, 9.835s/100 iters), loss = 0.00675588
I1112 01:36:26.811820  3508 solver.cpp:238]     Train net output #0: loss = 0.00675582 (* 1 = 0.00675582 loss)
I1112 01:36:26.811844  3508 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I1112 01:36:36.599386  3508 solver.cpp:219] Iteration 8300 (10.2176 iter/s, 9.787s/100 iters), loss = 0.0254998
I1112 01:36:36.599474  3508 solver.cpp:238]     Train net output #0: loss = 0.0254998 (* 1 = 0.0254998 loss)
I1112 01:36:36.599488  3508 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I1112 01:36:46.404641  3508 solver.cpp:219] Iteration 8400 (10.1989 iter/s, 9.805s/100 iters), loss = 0.0052498
I1112 01:36:46.404687  3508 solver.cpp:238]     Train net output #0: loss = 0.00524974 (* 1 = 0.00524974 loss)
I1112 01:36:46.404714  3508 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I1112 01:36:49.659649  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:36:56.077271  3508 solver.cpp:331] Iteration 8500, Testing net (#0)
I1112 01:37:01.797839  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:37:02.035045  3508 solver.cpp:398]     Test net output #0: accuracy = 0.992
I1112 01:37:02.035090  3508 solver.cpp:398]     Test net output #1: loss = 0.0255561 (* 1 = 0.0255561 loss)
I1112 01:37:02.127964  3508 solver.cpp:219] Iteration 8500 (6.36011 iter/s, 15.723s/100 iters), loss = 0.0048968
I1112 01:37:02.128006  3508 solver.cpp:238]     Train net output #0: loss = 0.00489673 (* 1 = 0.00489673 loss)
I1112 01:37:02.128033  3508 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I1112 01:37:11.880491  3508 solver.cpp:219] Iteration 8600 (10.2543 iter/s, 9.752s/100 iters), loss = 0.000251755
I1112 01:37:11.880544  3508 solver.cpp:238]     Train net output #0: loss = 0.000251671 (* 1 = 0.000251671 loss)
I1112 01:37:11.880571  3508 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I1112 01:37:21.668920  3508 solver.cpp:219] Iteration 8700 (10.2166 iter/s, 9.788s/100 iters), loss = 0.00251031
I1112 01:37:21.669193  3508 solver.cpp:238]     Train net output #0: loss = 0.00251022 (* 1 = 0.00251022 loss)
I1112 01:37:21.669234  3508 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I1112 01:37:31.569144  3508 solver.cpp:219] Iteration 8800 (10.102 iter/s, 9.899s/100 iters), loss = 0.000581369
I1112 01:37:31.569231  3508 solver.cpp:238]     Train net output #0: loss = 0.000581284 (* 1 = 0.000581284 loss)
I1112 01:37:31.569252  3508 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I1112 01:37:41.859113  3508 solver.cpp:219] Iteration 8900 (9.71912 iter/s, 10.289s/100 iters), loss = 0.000525623
I1112 01:37:41.859179  3508 solver.cpp:238]     Train net output #0: loss = 0.000525539 (* 1 = 0.000525539 loss)
I1112 01:37:41.859190  3508 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I1112 01:37:51.911659  3508 solver.cpp:331] Iteration 9000, Testing net (#0)
I1112 01:37:57.742581  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:37:57.962189  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9925
I1112 01:37:57.962232  3508 solver.cpp:398]     Test net output #1: loss = 0.0239541 (* 1 = 0.0239541 loss)
I1112 01:37:58.049486  3508 solver.cpp:219] Iteration 9000 (6.17665 iter/s, 16.19s/100 iters), loss = 0.011125
I1112 01:37:58.049535  3508 solver.cpp:238]     Train net output #0: loss = 0.0111249 (* 1 = 0.0111249 loss)
I1112 01:37:58.049573  3508 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I1112 01:38:07.981343  3508 solver.cpp:219] Iteration 9100 (10.0695 iter/s, 9.931s/100 iters), loss = 0.00449288
I1112 01:38:07.981392  3508 solver.cpp:238]     Train net output #0: loss = 0.0044928 (* 1 = 0.0044928 loss)
I1112 01:38:07.981402  3508 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I1112 01:38:18.271297  3508 solver.cpp:219] Iteration 9200 (9.71912 iter/s, 10.289s/100 iters), loss = 0.00300384
I1112 01:38:18.271349  3508 solver.cpp:238]     Train net output #0: loss = 0.00300375 (* 1 = 0.00300375 loss)
I1112 01:38:18.271360  3508 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I1112 01:38:28.687732  3508 solver.cpp:219] Iteration 9300 (9.60061 iter/s, 10.416s/100 iters), loss = 0.00537399
I1112 01:38:28.687914  3508 solver.cpp:238]     Train net output #0: loss = 0.00537389 (* 1 = 0.00537389 loss)
I1112 01:38:28.687933  3508 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I1112 01:38:35.407835  3518 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:38:38.115618  3508 solver.cpp:219] Iteration 9400 (10.6078 iter/s, 9.427s/100 iters), loss = 0.0138715
I1112 01:38:38.115665  3508 solver.cpp:238]     Train net output #0: loss = 0.0138714 (* 1 = 0.0138714 loss)
I1112 01:38:38.115692  3508 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I1112 01:38:47.097079  3508 solver.cpp:331] Iteration 9500, Testing net (#0)
I1112 01:38:52.441332  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:38:52.663162  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9913
I1112 01:38:52.663208  3508 solver.cpp:398]     Test net output #1: loss = 0.0277854 (* 1 = 0.0277854 loss)
I1112 01:38:52.750458  3508 solver.cpp:219] Iteration 9500 (6.8334 iter/s, 14.634s/100 iters), loss = 0.00233908
I1112 01:38:52.750494  3508 solver.cpp:238]     Train net output #0: loss = 0.00233899 (* 1 = 0.00233899 loss)
I1112 01:38:52.750521  3508 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I1112 01:39:02.926041  3508 solver.cpp:219] Iteration 9600 (9.82801 iter/s, 10.175s/100 iters), loss = 0.000942697
I1112 01:39:02.926262  3508 solver.cpp:238]     Train net output #0: loss = 0.000942602 (* 1 = 0.000942602 loss)
I1112 01:39:02.926286  3508 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I1112 01:39:12.768544  3508 solver.cpp:219] Iteration 9700 (10.1605 iter/s, 9.842s/100 iters), loss = 0.00397587
I1112 01:39:12.768595  3508 solver.cpp:238]     Train net output #0: loss = 0.00397578 (* 1 = 0.00397578 loss)
I1112 01:39:12.768607  3508 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I1112 01:39:22.582577  3508 solver.cpp:219] Iteration 9800 (10.1906 iter/s, 9.813s/100 iters), loss = 0.00963301
I1112 01:39:22.582653  3508 solver.cpp:238]     Train net output #0: loss = 0.00963292 (* 1 = 0.00963292 loss)
I1112 01:39:22.582666  3508 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I1112 01:39:32.411972  3508 solver.cpp:219] Iteration 9900 (10.174 iter/s, 9.829s/100 iters), loss = 0.00529083
I1112 01:39:32.412045  3508 solver.cpp:238]     Train net output #0: loss = 0.00529073 (* 1 = 0.00529073 loss)
I1112 01:39:32.412056  3508 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I1112 01:39:42.827769  3508 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I1112 01:39:42.839372  3508 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I1112 01:39:42.916149  3508 solver.cpp:311] Iteration 10000, loss = 0.00426176
I1112 01:39:42.916188  3508 solver.cpp:331] Iteration 10000, Testing net (#0)
I1112 01:39:49.051916  3519 data_layer.cpp:73] Restarting data prefetching from start.
I1112 01:39:49.291612  3508 solver.cpp:398]     Test net output #0: accuracy = 0.9922
I1112 01:39:49.291704  3508 solver.cpp:398]     Test net output #1: loss = 0.0233948 (* 1 = 0.0233948 loss)
I1112 01:39:49.291731  3508 solver.cpp:316] Optimization Done.
I1112 01:39:49.291764  3508 caffe.cpp:259] Optimization Done.
